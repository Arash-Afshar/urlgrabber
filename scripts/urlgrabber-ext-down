#! /usr/bin/python
#  A very simple external downloader

import time, os, errno, sys
from urlgrabber.grabber import \
    _readlines, URLGrabberOptions, _loads, \
    PyCurlFileObject, URLGrabError

def write(fmt, *arg):
    try: os.write(1, fmt % arg)
    except OSError, e:
        if e.args[0] != errno.EPIPE: raise
        sys.exit(1)

class ProxyProgress:
    def start(self, *d1, **d2):
        self.next_update = 0
    def update(self, _amount_read):
        t = time.time()
        if t < self.next_update: return
        self.next_update = t + 0.31
        write('%d %d\n', self._id, _amount_read)

def main():
    import signal
    signal.signal(signal.SIGINT, lambda n, f: sys.exit(1))
    cnt = 0
    while True:
        lines = _readlines(0)
        if not lines: break
        for line in lines:
            cnt += 1
            opts = URLGrabberOptions()
            opts._id = cnt
            for k in line.split(' '):
                k, v = k.split('=', 1)
                setattr(opts, k, _loads(v))
            if opts.progress_obj:
                opts.progress_obj = ProxyProgress()
                opts.progress_obj._id = cnt
            tm = time.time()
            try:
                fo = PyCurlFileObject(opts.url, opts.filename, opts)
                fo._do_grab()
                fo.fo.close()
                size = fo._amount_read
                dlsz = size - fo._reget_length
                ug_err = 'OK'
            except URLGrabError, e:
                size = dlsz = 0
                ug_err = '%d %s' % e.args
            write('%d %d %d %.3f %s\n', opts._id, size, dlsz, time.time() - tm, ug_err)

if __name__ == '__main__':
    main()
